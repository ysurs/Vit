{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "##### Testing mha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mha import muliheaded_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mha=muliheaded_attention(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the module list output. We based on the number of heads we pass, we will have that many linear layers.\n",
    "testing_mha.q_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2625,  0.2740, -0.2138,  0.6563],\n",
      "         [ 0.2145, -0.5587,  0.0124,  0.6617],\n",
      "         [ 1.4776,  0.0247,  0.1867,  0.4367],\n",
      "         [-0.4658, -1.9022,  1.7393,  1.5262],\n",
      "         [-0.6221, -0.2175,  0.5872, -0.6206]],\n",
      "\n",
      "        [[ 0.1950,  0.6210, -1.5737,  1.7337],\n",
      "         [-0.9654,  0.8480,  1.0328, -0.8368],\n",
      "         [-1.5797, -1.4777,  0.1693,  0.0318],\n",
      "         [ 0.4615,  1.2474,  0.1352,  0.2735],\n",
      "         [-0.1720,  1.0442,  0.9636,  1.7562]]])\n",
      "torch.Size([5, 4])\n",
      "tensor([[-0.1146, -0.6354],\n",
      "        [-0.0770, -0.6512],\n",
      "        [ 0.1110, -0.7642],\n",
      "        [-0.1764, -0.5866],\n",
      "        [-0.1533, -0.6099]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.2227, -0.0050],\n",
      "        [-1.2072, -0.0021],\n",
      "        [-1.1608,  0.0028],\n",
      "        [-1.2190,  0.0178],\n",
      "        [-1.0228,  0.0423]], grad_fn=<MmBackward0>)\n",
      "torch.Size([5, 4])\n",
      "tensor([[-0.4493, -0.4046],\n",
      "        [-0.7134, -0.2643],\n",
      "        [-0.5432, -0.3444],\n",
      "        [-0.4751, -0.3930],\n",
      "        [-0.5797, -0.3361]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.8509, -0.4479],\n",
      "        [-0.9687,  0.5559],\n",
      "        [-0.9974,  0.0249],\n",
      "        [-0.9951, -0.0444],\n",
      "        [-0.9823, -0.2094]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A random tensor representing 1 image, 5 patches and embedding size of each patch to be 4\n",
    "testing_tensor=torch.randn(2,5,4)\n",
    "print(testing_tensor)\n",
    "testing_mha(testing_tensor).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output says that 2 heads will perform calculations on 5 patches where embedding size is 4. Since we have 2 heads, the embedding will be broken down into size of 2 and 2 for each head."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "from vit_model import patchify\n",
    "\n",
    "# Simulating an input where we have 3 images, each has 3 color channels. Each image has shape 4,4\n",
    "testing_patchfy=torch.randn(3,3,4,4)\n",
    "\n",
    "# The output in this case implies, each image has been broken down into 4 patches. Each patch has embedding dim=12\n",
    "patchify(testing_patchfy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
