{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "##### Testing mha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mha import muliheaded_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mha=muliheaded_attention(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the module list output. We based on the number of heads we pass, we will have that many linear layers.\n",
    "testing_mha.q_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2625,  0.2740, -0.2138,  0.6563],\n",
      "         [ 0.2145, -0.5587,  0.0124,  0.6617],\n",
      "         [ 1.4776,  0.0247,  0.1867,  0.4367],\n",
      "         [-0.4658, -1.9022,  1.7393,  1.5262],\n",
      "         [-0.6221, -0.2175,  0.5872, -0.6206]],\n",
      "\n",
      "        [[ 0.1950,  0.6210, -1.5737,  1.7337],\n",
      "         [-0.9654,  0.8480,  1.0328, -0.8368],\n",
      "         [-1.5797, -1.4777,  0.1693,  0.0318],\n",
      "         [ 0.4615,  1.2474,  0.1352,  0.2735],\n",
      "         [-0.1720,  1.0442,  0.9636,  1.7562]]])\n",
      "torch.Size([5, 4])\n",
      "tensor([[-0.1146, -0.6354],\n",
      "        [-0.0770, -0.6512],\n",
      "        [ 0.1110, -0.7642],\n",
      "        [-0.1764, -0.5866],\n",
      "        [-0.1533, -0.6099]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.2227, -0.0050],\n",
      "        [-1.2072, -0.0021],\n",
      "        [-1.1608,  0.0028],\n",
      "        [-1.2190,  0.0178],\n",
      "        [-1.0228,  0.0423]], grad_fn=<MmBackward0>)\n",
      "torch.Size([5, 4])\n",
      "tensor([[-0.4493, -0.4046],\n",
      "        [-0.7134, -0.2643],\n",
      "        [-0.5432, -0.3444],\n",
      "        [-0.4751, -0.3930],\n",
      "        [-0.5797, -0.3361]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.8509, -0.4479],\n",
      "        [-0.9687,  0.5559],\n",
      "        [-0.9974,  0.0249],\n",
      "        [-0.9951, -0.0444],\n",
      "        [-0.9823, -0.2094]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A random tensor representing 1 image, 5 patches and embedding size of each patch to be 4\n",
    "testing_tensor=torch.randn(2,5,4)\n",
    "print(testing_tensor)\n",
    "testing_mha(testing_tensor).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output says that 2 heads will perform calculations on 5 patches where embedding size is 4. Since we have 2 heads, the embedding will be broken down into size of 2 and 2 for each head."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out patchify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 49, 16])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_model import patchify, ViT\n",
    "\n",
    "# Simulating an input where we have 3 images, each has 3 color channels. Each image has shape 4,4\n",
    "# testing_patchfy=torch.randn(3,3,4,4)\n",
    "testing_patchfy=torch.randn(7,1,28,28)\n",
    "\n",
    "# The output in this case implies, each image has been broken down into 4 patches. Each patch has embedding dim=12\n",
    "patchify(testing_patchfy,7).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Testing out Vit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m myvit\u001b[39m=\u001b[39mViT()\n\u001b[1;32m      2\u001b[0m \u001b[39m# myvit(testing_patchfy).shape\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m re\u001b[39m=\u001b[39mmyvit(testing_patchfy)\n",
      "File \u001b[0;32m~/miniforge3/envs/adapter_b/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/Vit/source/vit_model.py:116\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    112\u001b[0m cls_added_patch_sequence\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token,patch_sequence),dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m patch_sequence \u001b[39min\u001b[39;00m hidden_patches])\n\u001b[1;32m    115\u001b[0m positional_embedding\u001b[39m=\u001b[39mget_positional_embeddings(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_patches_per_row\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\n\u001b[0;32m--> 116\u001b[0m patches_with_positional_embedding\u001b[39m=\u001b[39mcls_added_patch_sequence\u001b[39m+\u001b[39;49mpositional_embedding\u001b[39m.\u001b[39;49mrepeat(images_per_batch,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m patches_with_positional_embedding\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "myvit=ViT()\n",
    "# myvit(testing_patchfy).shape\n",
    "re=myvit(testing_patchfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 2.0000, 1.0000,  ..., 2.0000, 1.0000, 2.0000],\n",
       "         [1.8415, 1.5403, 1.3110,  ..., 2.0000, 1.0003, 2.0000],\n",
       "         [1.9093, 0.5839, 1.5911,  ..., 2.0000, 1.0006, 2.0000],\n",
       "         ...,\n",
       "         [1.1236, 0.0077, 1.7481,  ..., 1.9989, 1.0149, 1.9999],\n",
       "         [0.2317, 0.3599, 1.5047,  ..., 1.9988, 1.0152, 1.9999],\n",
       "         [0.0462, 1.3006, 1.2112,  ..., 1.9988, 1.0155, 1.9999]],\n",
       "\n",
       "        [[1.0000, 2.0000, 1.0000,  ..., 2.0000, 1.0000, 2.0000],\n",
       "         [1.8415, 1.5403, 1.3110,  ..., 2.0000, 1.0003, 2.0000],\n",
       "         [1.9093, 0.5839, 1.5911,  ..., 2.0000, 1.0006, 2.0000],\n",
       "         ...,\n",
       "         [1.1236, 0.0077, 1.7481,  ..., 1.9989, 1.0149, 1.9999],\n",
       "         [0.2317, 0.3599, 1.5047,  ..., 1.9988, 1.0152, 1.9999],\n",
       "         [0.0462, 1.3006, 1.2112,  ..., 1.9988, 1.0155, 1.9999]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(re.repeat(2,1,1))+torch.ones(2,50,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 16])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,50,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cls=nn.Parameter(torch.ones(1,16))\n",
    "\n",
    "cls_input=[torch.cat((test_cls,p_s),dim=0) for p_s in re]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 16])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(cls_input)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvit.patch_embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
